{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8820277,"sourceType":"datasetVersion","datasetId":5306251}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import BertTokenizer\nfrom torch.nn import functional as F\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nfrom tqdm import tqdm\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom transformers import BertTokenizer, BertModel, AdamW","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-01T17:57:39.747027Z","iopub.execute_input":"2024-08-01T17:57:39.747415Z","iopub.status.idle":"2024-08-01T17:57:40.192975Z","shell.execute_reply.started":"2024-08-01T17:57:39.747385Z","shell.execute_reply":"2024-08-01T17:57:40.191886Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Load dataset\ndata = pd.read_excel('/kaggle/input/ubmec-bangla-dataset/UBMEC.xlsx')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-01T17:57:41.987751Z","iopub.execute_input":"2024-08-01T17:57:41.988607Z","iopub.status.idle":"2024-08-01T17:57:44.046624Z","shell.execute_reply.started":"2024-08-01T17:57:41.988572Z","shell.execute_reply":"2024-08-01T17:57:44.045833Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Preprocess the dataset\ndata = data.dropna(subset=['text'])  # Drop rows where 'text' column is NaN\ntexts = data['text'].astype(str).values  # Ensure all texts are strings\nlabels = data['classes'].values\n\n# Encode the labels\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(labels)\n\n# # Preprocess the dataset\n# texts = data['text'].values\n# labels = data['classes'].values\n\n# # Encode the labels\n# label_encoder = LabelEncoder()\n# labels = label_encoder.fit_transform(labels)\n\n# Train-test split\ntexts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Split training data further for validation\ntexts_train, texts_val, labels_train, labels_val = train_test_split(texts_train, labels_train, test_size=0.1, random_state=42)\n\n# Initialize the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\nclass BanglaDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-01T17:57:44.171022Z","iopub.execute_input":"2024-08-01T17:57:44.171840Z","iopub.status.idle":"2024-08-01T17:57:44.749262Z","shell.execute_reply.started":"2024-08-01T17:57:44.171809Z","shell.execute_reply":"2024-08-01T17:57:44.748266Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Create DataLoaders\nmax_length = 512\nbatch_size = 8\n\ntrain_dataset = BanglaDataset(texts_train, labels_train, tokenizer, max_length)\nval_dataset = BanglaDataset(texts_val, labels_val, tokenizer, max_length)\ntest_dataset = BanglaDataset(texts_test, labels_test, tokenizer, max_length)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nclass BertTextClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(BertTextClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[1]\n        output = self.dropout(pooled_output)\n        return self.fc(output)\n\n# Example model parameters\nnum_classes = len(label_encoder.classes_)\nmodel = BertTextClassifier(num_classes)\n\n# Training parameters\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Early stopping class\nclass EarlyStopping:\n    def __init__(self, patience=3, verbose=False):\n        self.patience = patience\n        self.verbose = verbose\n        self.best_loss = np.inf\n        self.counter = 0\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if val_loss < self.best_loss:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n                if self.verbose:\n                    print(f'Early stopping triggered after {self.counter} epochs with no improvement.')\n\n# Training loop with validation and early stopping\nepochs = 20\nearly_stopping = EarlyStopping(patience=3, verbose=True)\n\nfor epoch in range(epochs):\n    model.train()\n    for batch in tqdm(train_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch in tqdm(val_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask)\n            val_loss += criterion(outputs, labels).item()\n    \n    val_loss /= len(val_loader)  # Average validation loss\n\n    print(f'Epoch {epoch + 1}/{epochs} - Validation Loss: {val_loss}')\n\n    # Check early stopping\n    early_stopping(val_loss)\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break","metadata":{"execution":{"iopub.status.busy":"2024-08-01T17:57:49.117093Z","iopub.execute_input":"2024-08-01T17:57:49.117918Z","iopub.status.idle":"2024-08-01T18:45:18.371239Z","shell.execute_reply.started":"2024-08-01T17:57:49.117888Z","shell.execute_reply":"2024-08-01T18:45:18.370191Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edf0957b3de54dd281948fd3c883b943"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n100%|██████████| 1210/1210 [09:08<00:00,  2.21it/s]\n100%|██████████| 135/135 [00:18<00:00,  7.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 - Validation Loss: 1.34835280224129\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1210/1210 [09:11<00:00,  2.20it/s]\n100%|██████████| 135/135 [00:18<00:00,  7.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 - Validation Loss: 1.2453252037366231\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1210/1210 [09:11<00:00,  2.19it/s]\n100%|██████████| 135/135 [00:18<00:00,  7.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 - Validation Loss: 1.262376062075297\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1210/1210 [09:11<00:00,  2.19it/s]\n100%|██████████| 135/135 [00:18<00:00,  7.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20 - Validation Loss: 1.2954042295614878\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1210/1210 [09:11<00:00,  2.19it/s]\n100%|██████████| 135/135 [00:18<00:00,  7.33it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20 - Validation Loss: 1.5137458461302298\nEarly stopping triggered after 3 epochs with no improvement.\nEarly stopping\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluation before quantization\nmodel.to('cpu')\ndevice = 'cpu'\nmodel.eval()\ntest_preds = []\ntest_labels = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        outputs = model(input_ids, attention_mask)  # Pass both input_ids and attention_mask\n        test_preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n        test_labels.append(labels.cpu().numpy())\n\ntest_preds = np.concatenate(test_preds)\ntest_labels = np.concatenate(test_labels)\ntest_preds_class = np.argmax(test_preds, axis=1)\naccuracy = accuracy_score(test_labels, test_preds_class)\nrecall = recall_score(test_labels, test_preds_class, average='weighted')\nprecision = precision_score(test_labels, test_preds_class, average='weighted')\nf1 = f1_score(test_labels, test_preds_class, average='weighted')\nmicro_f1 = f1_score(test_labels, test_preds_class, average='micro')\nmacro_roc_auc = roc_auc_score(test_labels, test_preds, multi_class='ovo', average='macro')\n\nprint(f'Before Quantization - Accuracy: {accuracy}')\nprint(f'Before Quantization - Recall: {recall}')\nprint(f'Before Quantization - Precision: {precision}')\nprint(f'Before Quantization - F1: {f1}')\nprint(f'Before Quantization - Micro F1: {micro_f1}')\nprint(f'Before Quantization - Macro Roc Auc: {macro_roc_auc}')\n\n# # Evaluation on test set\n# model.eval()\n# test_loss = 0\n# correct_predictions = 0\n# with torch.no_grad():\n#     for batch in tqdm(test_loader):\n#         input_ids = batch['input_ids'].to(device)\n#         attention_mask = batch['attention_mask'].to(device)\n#         labels = batch['labels'].to(device)\n\n#         outputs = model(input_ids, attention_mask)\n#         test_loss += criterion(outputs, labels).item()\n#         _, preds = torch.max(outputs, dim=1)\n#         correct_predictions += torch.sum(preds == labels)\n\n# test_loss /= len(test_loader)  # Average test loss\n# accuracy = correct_predictions.double() / len(test_loader.dataset)\n# print(f'Test Loss: {test_loss}, Test Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-08-01T18:51:05.796057Z","iopub.execute_input":"2024-08-01T18:51:05.796916Z","iopub.status.idle":"2024-08-01T19:20:39.624635Z","shell.execute_reply.started":"2024-08-01T18:51:05.796882Z","shell.execute_reply":"2024-08-01T19:20:39.623710Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"100%|██████████| 336/336 [29:33<00:00,  5.28s/it]","output_type":"stream"},{"name":"stdout","text":"Before Quantization - Accuracy: 0.5252976190476191\nBefore Quantization - Recall: 0.5252976190476191\nBefore Quantization - Precision: 0.5534137028746428\nBefore Quantization - F1: 0.5332314913791076\nBefore Quantization - Micro F1: 0.5252976190476191\nBefore Quantization - Macro Roc Auc: 0.827190443483228\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Apply dynamic quantization\nquantized_model = torch.quantization.quantize_dynamic(\n    model,\n    {nn.Linear},\n    dtype=torch.qint8\n)\n\n# Evaluation after quantization\ntest_preds = []\ntest_labels = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        outputs = quantized_model(input_ids, attention_mask)  # Pass both input_ids and attention_mask\n        test_preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n        test_labels.append(labels.cpu().numpy())\n\ntest_preds = np.concatenate(test_preds)\ntest_labels = np.concatenate(test_labels)\ntest_preds_class = np.argmax(test_preds, axis=1)\naccuracy = accuracy_score(test_labels, test_preds_class)\nrecall = recall_score(test_labels, test_preds_class, average='weighted')\nprecision = precision_score(test_labels, test_preds_class, average='weighted')\nf1 = f1_score(test_labels, test_preds_class, average='weighted')\nmicro_f1 = f1_score(test_labels, test_preds_class, average='micro')\nmacro_roc_auc = roc_auc_score(test_labels, test_preds, multi_class='ovo', average='macro')\n\nprint(f'After Quantization - Accuracy: {accuracy}')\nprint(f'After Quantization - Recall: {recall}')\nprint(f'After Quantization - Precision: {precision}')\nprint(f'After Quantization - F1: {f1}')\nprint(f'After Quantization - Micro F1: {micro_f1}')\nprint(f'After Quantization - Macro Roc Auc: {macro_roc_auc}')","metadata":{"execution":{"iopub.status.busy":"2024-08-01T19:21:42.765768Z","iopub.execute_input":"2024-08-01T19:21:42.766121Z","iopub.status.idle":"2024-08-01T19:49:25.932144Z","shell.execute_reply.started":"2024-08-01T19:21:42.766093Z","shell.execute_reply":"2024-08-01T19:49:25.931205Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"100%|██████████| 336/336 [27:41<00:00,  4.95s/it]","output_type":"stream"},{"name":"stdout","text":"After Quantization - Accuracy: 0.5219494047619048\nAfter Quantization - Recall: 0.5219494047619048\nAfter Quantization - Precision: 0.5477307224560475\nAfter Quantization - F1: 0.5273914640448607\nAfter Quantization - Micro F1: 0.5219494047619048\nAfter Quantization - Macro Roc Auc: 0.8261630369547507\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}